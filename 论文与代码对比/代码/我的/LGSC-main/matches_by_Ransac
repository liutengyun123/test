import cv2
import numpy as np
import testmodel

from enum import Enum


class DrawingType(Enum):
    ONLY_LINES = 1
    LINES_AND_POINTS = 2
    COLOR_CODED_POINTS_X = 3
    COLOR_CODED_POINTS_Y = 4
    COLOR_CODED_POINTS_XpY = 5
 
def siftandgetlocation(img1,img2,goodflag=1):
    sift = cv2.SIFT_create(500)

    # find the keypoints and descriptors with SIFT
    loc1, des1 = sift.detectAndCompute(img1, None)
    loc2, des2 = sift.detectAndCompute(img2, None)
    # abb = cv2.drawKeypoints(img2, loc2, img2, (255, 0, 255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)  #
    # cv2.imshow('abb', abb)
    # FLANN parameters
    FLANN_INDEX_KDTREE = 0
    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
    search_params = dict()
    flann = cv2.FlannBasedMatcher(index_params, search_params)
    matches = flann.knnMatch(des1, des2, k=2)

    # Apply ratio test
    good = []
    for m, n in matches:
        if m.distance < 0.7 * n.distance:
            good.append(m)
    print('num of matching points', len(good))

    return [good,loc1,loc2]


def draw_matches(src1, src2, kp1, kp2, matches, drawing_type):
    height = max(src1.shape[0], src2.shape[0])
    width = src1.shape[1] + src2.shape[1]
    output = np.zeros((height, width, 3), dtype=np.uint8)
    output[0:src1.shape[0], 0:src1.shape[1]] = src1
    output[0:src2.shape[0], src1.shape[1]:] = src2[:]
 
    if drawing_type == DrawingType.ONLY_LINES:
        for i in range(len(matches)):
            left = kp1[matches[i].queryIdx].pt
            right = tuple(sum(x) for x in zip(kp2[matches[i].trainIdx].pt, (src1.shape[1], 0)))
            cv2.line(output, tuple(map(int, left)), tuple(map(int, right)), (0, 255, 255))
 
    elif drawing_type == DrawingType.LINES_AND_POINTS:
        for i in range(len(matches)):
            left = kp1[matches[i].queryIdx].pt
            right = tuple(sum(x) for x in zip(kp2[matches[i].trainIdx].pt, (src1.shape[1], 0)))
            cv2.line(output, tuple(map(int, left)), tuple(map(int, right)), (255, 0, 0))
 
        for i in range(len(matches)):
            left = kp1[matches[i].queryIdx].pt
            right = tuple(sum(x) for x in zip(kp2[matches[i].trainIdx].pt, (src1.shape[1], 0)))
            cv2.circle(output, tuple(map(int, left)), 1, (0, 255, 255), 2)
            cv2.circle(output, tuple(map(int, right)), 1, (0, 255, 0), 2)
 
    elif drawing_type == DrawingType.COLOR_CODED_POINTS_X or drawing_type == DrawingType.COLOR_CODED_POINTS_Y or drawing_type == DrawingType.COLOR_CODED_POINTS_XpY:
        _1_255 = np.expand_dims(np.array(range(0, 256), dtype='uint8'), 1)
        _colormap = cv2.applyColorMap(_1_255, cv2.COLORMAP_HSV)
 
        for i in range(len(matches)):
            left = kp1[matches[i].queryIdx].pt
            right = tuple(sum(x) for x in zip(kp2[matches[i].trainIdx].pt, (src1.shape[1], 0)))
 
            if drawing_type == DrawingType.COLOR_CODED_POINTS_X:
                colormap_idx = int(left[0] * 256. / src1.shape[1])  # x-gradient
            if drawing_type == DrawingType.COLOR_CODED_POINTS_Y:
                colormap_idx = int(left[1] * 256. / src1.shape[0])  # y-gradient
            if drawing_type == DrawingType.COLOR_CODED_POINTS_XpY:
                colormap_idx = int((left[0] - src1.shape[1]*.5 + left[1] - src1.shape[0]*.5) * 256. / (src1.shape[0]*.5 + src1.shape[1]*.5))  # manhattan gradient
 
            color = tuple(map(int, _colormap[colormap_idx, 0, :]))
            cv2.circle(output, tuple(map(int, left)), 1, color, 2)
            cv2.circle(output, tuple(map(int, right)), 1, color, 2)
    return output


ori_img = cv2.imread('D:\googleDownload\hpatches-sequences-release\hpatches-sequences-release\\i_yellowtent\\1.ppm', 0)
dst_img = cv2.imread('D:\googleDownload\hpatches-sequences-release\hpatches-sequences-release\\i_yellowtent\\3.ppm', 0)

# ori_img = cv2.imread('D:\googleDownload\hpatches-sequences-release\hpatches-sequences-release\\v_yard\\1.ppm', 0)
# dst_img = cv2.imread('D:\googleDownload\hpatches-sequences-release\hpatches-sequences-release\\v_yard\\3.ppm', 0)

result = siftandgetlocation(ori_img, dst_img, 1)
matches = result[0]
ori_kp = result[1]
dst_kp = result[2]

Hlist1_3 = [[1,0, 0],[0, 1, 0],[0, 0, 1]]     
# Hlist1_3 = [[0.81916, -0.19602, -221.9],[-0.18509, 0.99057, -55.575],[-0.00033371, -0.00011673, 0.99377]]
# Hlist1_3 = [[0.80989,0.15593, -274.24],[-0.19006, 1.3082, -274.15],[-0.00047728, 0.0001622, 1]]                              #v_yuri 1-3
Hlist1_4 = [[2.4144,-0.0022023,-199.3],[0.52146,2.0547,-569.49],[0.0010423,8.4489e-05,1.0043]]                                #v_london   1_4
H = np.array(Hlist1_3)


print('the correct matches before removal:')
before,count_before = testmodel.testscoreimage(matches, ori_kp, dst_kp, H, ori_img, dst_img)
outimg=cv2.drawMatches(ori_img,ori_kp,dst_img,dst_kp,matches,None,flags=2)
cv2.imshow("before",before)

# 计算匹配点的数量
num_matches = len(matches)
print("特征匹配点数量：", num_matches)

# 使用RANSAC算法进行误匹配的剔除
src_pts = np.float32([ori_kp[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
dst_pts = np.float32([dst_kp[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)
M, mask = cv2.findHomography(src_pts, dst_pts, cv2.USAC_MAGSAC,5)
# M, mask = cv2.findHomography(src_pts, dst_pts, cv2.USAC_PROSAC,5)
#M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5)

# 剔除误匹配的特征点
good_matches = [matches[i] for i, m in enumerate(mask) if m]

print("ORB+RANSAC特征点数量：",len(good_matches))

# 绘制匹配结果
img4 = cv2.drawMatches(ori_img, ori_kp, dst_img, dst_kp, good_matches, None, flags=2)

# 显示结果
cv2.imshow('all Matches', outimg)
    
cv2.imshow("SAC Matches", img4)

#通过H矩阵计算正确的匹配点对
summ = testmodel.testscore(good_matches, ori_kp, dst_kp, H)
print('the number of correct matches after removal:', summ)
after,count_after = testmodel.testscoreimage(good_matches, ori_kp, dst_kp, H,ori_img,dst_img)
cv2.imshow('after', after)

recall = count_after/count_before
precision = count_after/len(good_matches)
F_score = 2*recall*precision/(recall+precision)

print("召回率：",recall)
print("准确率：",precision)
print("F-score:",F_score)

cv2.waitKey()